<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Membership Inference Attacks Against NLP Classification Models | Chengyu Zhang</title>
<meta name="keywords" content="Membership Inference Attack, NLP">
<meta name="description" content="A comprehensive analysis of privacy risks in NLP classification models, focusing on membership inference attacks (MIAs) at both sample and user levels.">
<meta name="author" content="Chengyu Zhang">
<link rel="canonical" href="https://gnehcuyz.github.io/reading/membership_inference_attacks_nlp_classification_models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '\\(', right: '\\)', display: false},  
      ],
      throwOnError : false
    });
  });
</script>
  
<link rel="icon" href="https://gnehcuyz.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://gnehcuyz.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://gnehcuyz.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://gnehcuyz.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://gnehcuyz.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://gnehcuyz.github.io/reading/membership_inference_attacks_nlp_classification_models/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://gnehcuyz.github.io/reading/membership_inference_attacks_nlp_classification_models/">
  <meta property="og:site_name" content="Chengyu Zhang">
  <meta property="og:title" content="Membership Inference Attacks Against NLP Classification Models">
  <meta property="og:description" content="A comprehensive analysis of privacy risks in NLP classification models, focusing on membership inference attacks (MIAs) at both sample and user levels.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="reading">
    <meta property="article:published_time" content="2023-09-19T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-09-19T00:00:00+00:00">
    <meta property="article:tag" content="Membership Inference Attack">
    <meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Membership Inference Attacks Against NLP Classification Models">
<meta name="twitter:description" content="A comprehensive analysis of privacy risks in NLP classification models, focusing on membership inference attacks (MIAs) at both sample and user levels.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Reading",
      "item": "https://gnehcuyz.github.io/reading/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Membership Inference Attacks Against NLP Classification Models",
      "item": "https://gnehcuyz.github.io/reading/membership_inference_attacks_nlp_classification_models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Membership Inference Attacks Against NLP Classification Models",
  "name": "Membership Inference Attacks Against NLP Classification Models",
  "description": "A comprehensive analysis of privacy risks in NLP classification models, focusing on membership inference attacks (MIAs) at both sample and user levels.",
  "keywords": [
    "Membership Inference Attack", "NLP"
  ],
  "articleBody": "Overview This paper addresses privacy concerns in NLP classification models, focusing on how these models can leak private information through Membership Inference Attacks (MIAs). It explores both sample-level MIAs and introduces user-level MIAs, demonstrating the latter’s stronger privacy risks due to aggregated data features.\nContributions The study makes three key contributions:\nA systematic analysis of privacy risks in NLP classification models. A suite of MIA methods for sample-level and novel user-level inferences. Experimental results showcasing user-level MIAs’ heightened accuracy (up to 25% better than sample-level MIAs). Methodology Sample-Level MIAs These attacks infer if a single data sample was part of the model’s training data. Features such as cross-entropy loss, rank, and confidence are used, with a threshold distinguishing members from non-members.\nUser-Level MIAs Unlike sample-level, user-level MIAs aggregate information from multiple samples tied to a user. This study proposed three types:\nThreshold-based: Using average statistics of user data features. Learning-based: Leveraging machine learning models trained on aggregated user features. Sample-to-user-level: Combining individual sample predictions to infer user membership. Experimental Findings Using datasets like Reddit and Amazon reviews, the experiments highlight:\nUser-level MIAs are more effective than sample-level, especially with Transformer-based models like BERT. Privacy risks increase with more classes, larger vocabularies, and longer token sequences. User contributions across multiple data classes increase their susceptibility. Implications The research emphasizes the need for stronger privacy measures in NLP models, as current practices are insufficient to prevent MIAs. Future directions include:\nAuditing data collection processes using MIAs. Developing text perturbation methods to mitigate privacy risks. Reference Shejwalkar, V., Inan, H. A., Houmansadr, A., \u0026 Sim, R. (2021). Membership Inference Attacks Against NLP Classification Models. Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Read the full paper.\n",
  "wordCount" : "287",
  "inLanguage": "en",
  "datePublished": "2023-09-19T00:00:00Z",
  "dateModified": "2023-09-19T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Chengyu Zhang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://gnehcuyz.github.io/reading/membership_inference_attacks_nlp_classification_models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Chengyu Zhang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://gnehcuyz.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://gnehcuyz.github.io/" accesskey="h" title="Chengyu Zhang (Alt + H)">Chengyu Zhang</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://gnehcuyz.github.io/about/" title="About me">
                    <span>About me</span>
                </a>
            </li>
            <li>
                <a href="https://gnehcuyz.github.io/experience" title="Experience">
                    <span>Experience</span>
                </a>
            </li>
            <li>
                <a href="https://gnehcuyz.github.io/research/" title="Research">
                    <span>Research</span>
                </a>
            </li>
            <li>
                <a href="https://gnehcuyz.github.io/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="https://gnehcuyz.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://gnehcuyz.github.io/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://gnehcuyz.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://gnehcuyz.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://gnehcuyz.github.io/reading/">Reading</a></div>
    <h1 class="post-title entry-hint-parent">
      Membership Inference Attacks Against NLP Classification Models
    </h1>
    <div class="post-meta"><span title='2023-09-19 00:00:00 +0000 UTC'>September 19, 2023</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Chengyu Zhang

</div>
  </header> 
  <div class="post-content"><h3 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h3>
<p>This paper addresses privacy concerns in NLP classification models, focusing on how these models can leak private information through Membership Inference Attacks (MIAs). It explores both <strong>sample-level MIAs</strong> and introduces <strong>user-level MIAs</strong>, demonstrating the latter&rsquo;s stronger privacy risks due to aggregated data features.</p>
<h3 id="contributions">Contributions<a hidden class="anchor" aria-hidden="true" href="#contributions">#</a></h3>
<p>The study makes three key contributions:</p>
<ol>
<li>A systematic analysis of privacy risks in NLP classification models.</li>
<li>A suite of MIA methods for sample-level and novel user-level inferences.</li>
<li>Experimental results showcasing user-level MIAs&rsquo; heightened accuracy (up to 25% better than sample-level MIAs).</li>
</ol>
<h3 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h3>
<h4 id="sample-level-mias">Sample-Level MIAs<a hidden class="anchor" aria-hidden="true" href="#sample-level-mias">#</a></h4>
<p>These attacks infer if a single data sample was part of the model&rsquo;s training data. Features such as cross-entropy loss, rank, and confidence are used, with a threshold distinguishing members from non-members.</p>
<h4 id="user-level-mias">User-Level MIAs<a hidden class="anchor" aria-hidden="true" href="#user-level-mias">#</a></h4>
<p>Unlike sample-level, user-level MIAs aggregate information from multiple samples tied to a user. This study proposed three types:</p>
<ul>
<li><strong>Threshold-based:</strong> Using average statistics of user data features.</li>
<li><strong>Learning-based:</strong> Leveraging machine learning models trained on aggregated user features.</li>
<li><strong>Sample-to-user-level:</strong> Combining individual sample predictions to infer user membership.</li>
</ul>
<h3 id="experimental-findings">Experimental Findings<a hidden class="anchor" aria-hidden="true" href="#experimental-findings">#</a></h3>
<p>Using datasets like Reddit and Amazon reviews, the experiments highlight:</p>
<ul>
<li>User-level MIAs are more effective than sample-level, especially with Transformer-based models like BERT.</li>
<li>Privacy risks increase with more classes, larger vocabularies, and longer token sequences.</li>
<li>User contributions across multiple data classes increase their susceptibility.</li>
</ul>
<h3 id="implications">Implications<a hidden class="anchor" aria-hidden="true" href="#implications">#</a></h3>
<p>The research emphasizes the need for stronger privacy measures in NLP models, as current practices are insufficient to prevent MIAs. Future directions include:</p>
<ul>
<li>Auditing data collection processes using MIAs.</li>
<li>Developing text perturbation methods to mitigate privacy risks.</li>
</ul>
<h3 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h3>
<p>Shejwalkar, V., Inan, H. A., Houmansadr, A., &amp; Sim, R. (2021). Membership Inference Attacks Against NLP Classification Models. <em>Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021)</em>. <a href="https://arxiv.org/abs/2111.07960">Read the full paper</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://gnehcuyz.github.io/tags/Membership-Inference-Attack/">Membership Inference Attack</a></li>
      <li><a href="https://gnehcuyz.github.io/tags/NLP/">NLP</a></li>
    </ul>
  </footer>
</article>
    </main>
    <footer style="text-align: center; padding: 1rem 0; font-size: 0.9rem;">
    <p>
        Last updated on 2025-05-27 ||
        &copy; 2025 
        <a href="https://yourwebsite.com" style="text-decoration: underline;">Chengyu Zhang</a> ||
        Powered by 
        <a href="https://gohugo.io/" style="text-decoration: underline;">Hugo</a> & 
        <a href="https://themes.gohugo.io/themes/hugo-papermod/" style="text-decoration: underline;">PaperMod</a>
    </p>
</footer>
</body>

</html>
